{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3854030d-c485-448a-a570-49b1f2821a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a965d4-9c6a-4320-b091-992020cb183d",
   "metadata": {},
   "source": [
    "Load Text Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab69681-24b6-42bd-9f44-ff4272aec886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "DOCS_FOLDER = \"docs\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(DOCS_FOLDER):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(DOCS_FOLDER, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c5274-bfc2-4768-9cf2-d5936a486d98",
   "metadata": {},
   "source": [
    "Chunk Documents (200â€“300 Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4121ccaf-c8df-499f-9436-54dc332f826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks Created: 2\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "print(f\"Total Chunks Created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b811eea-aebe-4d64-bc4a-8bd407d000df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbc94c5f8654c0db861380aa89197ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293cce74969c4438a794c06b9f6415aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6ed97cc5a748f0b701bcc6fe02942d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e1d29ad1b747948d277fd40e64b56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddf03c1bcd9455aa55d364d1f03ce3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4f34d64b4e47eeb8de45a8409d6bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f6099b2fd2498eb1d1bbab5560de72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35c1d8daa67481092443ab59749085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07afba5071e847aab38d7dfca6f593f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cedc2fb1ab4062b72098220b616827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d333cfe0cb1d4a758324ae8c57d4e90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f77c8f403354d7eba47d660919bfd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chunk_embeddings = embedder.encode(\n",
    "    chunks,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f19ff2-0f93-4551-b6fa-108e33e9f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_chunks(question, top_k=3):\n",
    "    question_embedding = embedder.encode([question], convert_to_numpy=True)\n",
    "    \n",
    "    similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return [chunks[i] for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59afad89-6279-4a21-9376-763f76c18d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama(prompt):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"llama3.2\"],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    return result.stdout.decode(\"utf-8\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8472732e-3730-4afc-85ae-065b3df94db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_no_rag(question):\n",
    "    prompt = f\"Answer the question clearly and short:\\n{question}\"\n",
    "    return ask_llama(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd48a15-0e59-4bf0-b8aa-955af67b9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_with_rag(question):\n",
    "    top_chunks = retrieve_top_chunks(question)\n",
    "    \n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return ask_llama(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b026ade9-3992-401c-beb4-317ea149c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is Artificial Intelligence?\",\n",
    "    \"What is Natural Language Processing?\",\n",
    "    \"What are Large Language Models?\",\n",
    "    \"What is hallucination in LLMs?\",\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"Why is chunking used in RAG systems?\",\n",
    "    \"What are embeddings?\",\n",
    "    \"How does similarity search work?\",\n",
    "    \"Why does RAG reduce hallucinations?\",\n",
    "    \"What are applications of RAG?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef05c132-ce52-4918-86cc-d2c825735108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as:\n",
      "\n",
      "1. Learning: AI systems can learn from data and improve their performance over time.\n",
      "2. Problem-solving: AI systems can solve complex problems by analyzing data, identifying patterns, and making decisions.\n",
      "3. Reasoning: AI systems can draw conclusions based on the information they have been trained on.\n",
      "4. Perception: AI systems can interpret and understand data from sensors, such as images, speech, and text.\n",
      "\n",
      "AI has many applications in various fields, including:\n",
      "\n",
      "1. Virtual assistants (e.g., Siri, Alexa)\n",
      "2. Image recognition and object detection\n",
      "3. Natural Language Processing (NLP) for language translation and sentiment analysis\n",
      "4. Predictive analytics and decision-making\n",
      "5. Robotics and autonomous vehicles\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. Narrow or Weak AI: Designed to perform a specific task, such as facial recognition or language translation.\n",
      "2. General or Strong AI: Aims to create a computer system that can perform any intellectual task that humans can.\n",
      "3. Superintelligence: Significantly more intelligent than the best human minds.\n",
      "\n",
      "The benefits of AI include:\n",
      "\n",
      "1. Increased efficiency and productivity\n",
      "2. Improved accuracy and reliability\n",
      "3. Enhanced decision-making and problem-solving capabilities\n",
      "4. Personalization and customization\n",
      "\n",
      "However, there are also concerns and challenges associated with AI, such as:\n",
      "\n",
      "1. Job displacement and automation\n",
      "2. Bias and fairness in AI systems\n",
      "3. Cybersecurity risks and vulnerabilities\n",
      "4. Ethical considerations and accountability.\n",
      "\n",
      "Overall, Artificial Intelligence is a rapidly evolving field that has the potential to transform many aspects of our lives and society.\n"
     ]
    }
   ],
   "source": [
    "print(ask_llama(\"What is Artificial Intelligence?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b361f414-3266-4b58-8ed1-94b4e74de7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What is Artificial Intelligence?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "1. Learning: The ability to learn from data and improve performance over time.\n",
      "2. Problem-solving: The ability to analyze problems and find solutions.\n",
      "3. Reasoning: The ability to draw conclusions based on available information.\n",
      "4. Perception: The ability to interpret and understand data from sensors.\n",
      "\n",
      "AI systems use algorithms, machine learning models, and other techniques to process and analyze data, making decisions, and taking actions autonomously or with human oversight.\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. Narrow or Weak AI: Designed to perform a specific task, such as facial recognition or language translation.\n",
      "2. General or Strong AI: A hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\n",
      "3. Superintelligence: An AI system that significantly surpasses human intelligence in a broad domain.\n",
      "\n",
      "AI has many applications, including:\n",
      "\n",
      "1. Virtual assistants (e.g., Siri, Alexa)\n",
      "2. Image recognition and object detection\n",
      "3. Natural language processing (e.g., chatbots, language translation)\n",
      "4. Predictive maintenance and quality control\n",
      "5. Autonomous vehicles and robotics\n",
      "\n",
      "Overall, AI aims to create intelligent machines that can assist humans in various aspects of life, while also potentially driving innovation and solving complex problems.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are designed to think and act like humans.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What is Natural Language Processing?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It involves the development of algorithms, statistical models, and machine learning techniques to enable computers to understand, interpret, and generate human language.\n",
      "\n",
      "The primary goals of NLP are:\n",
      "\n",
      "1. **Text Analysis**: To analyze and extract meaningful information from unstructured text data.\n",
      "2. **Language Understanding**: To comprehend the meaning and intent behind human language.\n",
      "3. **Machine Translation**: To translate text from one language to another.\n",
      "4. **Sentiment Analysis**: To determine the emotional tone or sentiment of a piece of text.\n",
      "5. **Dialogue Systems**: To create computers that can engage in conversation with humans.\n",
      "\n",
      "NLP techniques include:\n",
      "\n",
      "1. **Tokenization**: Breaking down text into individual words or tokens.\n",
      "2. **Part-of-speech tagging**: Identifying the grammatical category of each word (e.g., noun, verb, adjective).\n",
      "3. **Named entity recognition**: Identifying specific entities mentioned in text (e.g., people, places, organizations).\n",
      "4. **Sentiment analysis**: Determining the emotional tone or sentiment of a piece of text.\n",
      "5. **Machine learning**: Using machine learning algorithms to improve NLP models.\n",
      "\n",
      "NLP has numerous applications in areas such as:\n",
      "\n",
      "1. Virtual assistants (e.g., Siri, Alexa)\n",
      "2. Chatbots\n",
      "3. Language translation software\n",
      "4. Sentiment analysis for customer feedback\n",
      "5. Text summarization and generation\n",
      "\n",
      "Overall, NLP is a rapidly evolving field that enables computers to understand and interact with human language more effectively.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "Natural Language Processing (NLP) refers to enabling machines to understand, interpret, and generate human language.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What are Large Language Models?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that uses machine learning algorithms to process and understand human language. These models are trained on vast amounts of text data, which enables them to learn patterns, relationships, and structures within language.\n",
      "\n",
      "Large Language Models are typically characterized by their:\n",
      "\n",
      "1. **Size**: They have billions of parameters, which represent the connections between different pieces of information in the model.\n",
      "2. **Training data**: They are trained on enormous amounts of text data, often generated from various sources such as books, articles, and online content.\n",
      "3. **Architecture**: They use complex neural network architectures to process input text and generate output.\n",
      "\n",
      "The primary goals of Large Language Models include:\n",
      "\n",
      "1. **Language understanding**: To comprehend the meaning and context of human language.\n",
      "2. **Text generation**: To produce coherent and natural-sounding text based on a given prompt or input.\n",
      "3. **Dialogue systems**: To engage in conversations with humans, answering questions and responding to statements.\n",
      "\n",
      "Large Language Models have many applications, including:\n",
      "\n",
      "1. **Chatbots and virtual assistants**\n",
      "2. **Language translation**\n",
      "3. **Text summarization**\n",
      "4. **Sentiment analysis**\n",
      "5. **Content creation**\n",
      "\n",
      "Examples of Large Language Models include BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, and transformers-based models like XLNet and T5.\n",
      "\n",
      "These models have revolutionized the field of natural language processing (NLP) and continue to improve the capabilities of AI systems in understanding and generating human language.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "According to the provided context, Large Language Models (LLMs) are advanced Natural Language Processing (NLP) models trained on massive amounts of text data. They can generate human-like responses but may sometimes produce incorrect information, a problem known as hallucination.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What is hallucination in LLMs?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "In Large Language Models (LLMs), a hallucination refers to a situation where the model generates incorrect or nonsensical information in response to a query or prompt, often drawing from its own internal knowledge graph or generating text that is not supported by the input data. This can occur when the model:\n",
      "\n",
      "1. Lack of relevant training data: If the training dataset lacks specific examples or context, the model may generate answers that are not accurate.\n",
      "2. Overfitting to patterns: The model might be overfitting to certain patterns in the training data, which can lead to incorrect or irrelevant information being generated.\n",
      "3. Model limitations: Some LLMs have inherent limitations in their ability to understand nuances of language, context, and common sense.\n",
      "\n",
      "Hallucinations are a concern in LLMs as they can:\n",
      "\n",
      "1. Spread misinformation\n",
      "2. Provide unhelpful or confusing answers\n",
      "3. Erode trust in AI-generated content\n",
      "\n",
      "Researchers and developers strive to mitigate hallucinations by improving the quality and diversity of training data, using techniques like adversarial training, and incorporating common sense and domain-specific knowledge into the models.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "In Large Language Models (LLMs), hallucination refers to the generation of incorrect information.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What is Retrieval-Augmented Generation?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Retrieval-Augmented Generation (RAG) is a technique used in natural language processing (NLP) that combines the strengths of both retrieval and generation models. The primary goal of RAG is to improve the performance of text generation tasks by leveraging external knowledge bases or databases to retrieve relevant information before generating new text.\n",
      "\n",
      "In traditional generation models, the generator produces text from scratch using a learned model. In contrast, RAG first retrieves relevant documents or information from an external knowledge base using a retrieval model. The retrieved information is then used as input to a generation model, which generates new text based on the retrieved knowledge.\n",
      "\n",
      "RAG has several key benefits, including:\n",
      "\n",
      "1. Improved accuracy: By leveraging external knowledge bases, RAG can generate more accurate and informative text.\n",
      "2. Increased efficiency: RAG can reduce the computational cost of generating text by using pre-computed retrieval results.\n",
      "3. Better handling of out-of-vocabulary words: RAG can handle out-of-vocabulary words by retrieving relevant definitions or explanations from the knowledge base.\n",
      "\n",
      "The overall architecture of an RAG system typically consists of two main components:\n",
      "\n",
      "1. Retrieval component: This uses a retrieval model to search for relevant documents or information in the external knowledge base.\n",
      "2. Generation component: This uses a generation model, such as a language model or a sequence-to-sequence model, to generate new text based on the retrieved information.\n",
      "\n",
      "RAG has been applied in various NLP tasks, including text generation, question answering, and summarization.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "Retrieval-Augmented Generation (RAG) is a method that combines information retrieval with text generation to improve factual accuracy by reducing hallucinations in Large Language Models (LLMs).\n",
      "\n",
      "================================================================================\n",
      "QUESTION: Why is chunking used in RAG systems?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Chunking is used in RAG (Resource Allocation Graph) systems to improve resource allocation efficiency and reduce complexity. In a RAG system, resources are represented as nodes, and tasks or jobs are represented as arcs that connect these nodes.\n",
      "\n",
      "Chunking involves dividing tasks into smaller sub-tasks or \"chunks\" that can be executed independently by multiple processors. By doing so, the system can:\n",
      "\n",
      "1. Increase parallelism: Chunking allows multiple chunks of a task to be processed simultaneously, increasing overall system throughput.\n",
      "2. Reduce processing time: Smaller tasks can be completed faster, reducing overall processing time.\n",
      "3. Improve scalability: As the number of resources increases, chunking enables the system to handle more tasks and increase its capacity.\n",
      "\n",
      "By using chunking in RAG systems, administrators can optimize resource allocation, improve system responsiveness, and enhance overall efficiency.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "Chunking is used in RAG systems to divide documents into smaller, manageable pieces called chunks (200-300 words). These chunks are then converted into numerical representations called embeddings, which capture semantic meaning. The primary purpose of chunking in RAG systems is to retrieve the most relevant context from a large corpus of text when a user asks a question, thereby reducing hallucinations and improving reliability.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What are embeddings?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Embeddings are a fundamental concept in natural language processing (NLP) and machine learning. In essence, an embedding is a numerical representation of a word or token that captures its meaning and context within a text.\n",
      "\n",
      "In traditional representations, such as one-hot encoding or bag-of-words, words are represented by vectors that indicate their presence or absence in a document. However, this approach is limited because it doesn't capture the nuances of language.\n",
      "\n",
      "Embeddings, on the other hand, are learned vector spaces where each word is mapped to a unique point in a high-dimensional space (typically hundreds or thousands of dimensions). These vectors can be used as input features for machine learning algorithms, such as neural networks.\n",
      "\n",
      "The main goals of embeddings are:\n",
      "\n",
      "1. **Capture semantic relationships**: Embeddings aim to preserve the meaning and context of words within a text.\n",
      "2. **Reduce dimensionality**: By representing words in a lower-dimensional space, embeddings reduce the complexity of NLP tasks.\n",
      "3. **Enable efficient computations**: Using embeddings as input features simplifies the computation of NLP tasks, such as word similarity and sentiment analysis.\n",
      "\n",
      "Common types of embeddings include:\n",
      "\n",
      "1. Word2Vec\n",
      "2. GloVe\n",
      "3. FastText\n",
      "\n",
      "These techniques have revolutionized many NLP applications, such as language translation, text classification, and question answering.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "In a Retrieval-Augmented Generation (RAG) system, embeddings are numerical representations of text chunks that capture their semantic meaning. They are typically created using models such as SentenceTransformers and are used to compare with document embeddings when a user asks a question.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: How does similarity search work?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "Similarity search is a type of search algorithm that finds the most similar items or objects to a given query or reference item in a dataset. Here's how it works:\n",
      "\n",
      "1. **Indexing**: The dataset is first indexed, which involves creating a data structure that allows for efficient querying. This indexing can be done using various techniques such as:\n",
      "\t* Inverted Index: A dictionary-like data structure where each key is a term (e.g., word or feature) and the value is a list of documents that contain that term.\n",
      "\t* Vector Space Model (VSM): A mathematical model that represents documents or items as vectors in a high-dimensional space, where similar items are close together.\n",
      "2. **Query Preparation**: When a query is received, it's typically transformed into a numerical representation using techniques such as:\n",
      "\t* Bag-of-Words: Representing the query as a vector of word frequencies.\n",
      "\t* Term Frequency-Inverse Document Frequency (TF-IDF): Weighting the importance of each word in the query based on its frequency and rarity across the dataset.\n",
      "3. **Distance Metric**: A distance metric is chosen to measure the similarity between the query and potential matches. Common distance metrics include:\n",
      "\t* Euclidean Distance: Measures the straight-line distance between two points in a high-dimensional space.\n",
      "\t* Cosine Similarity: Measures the angle between two vectors using their dot product.\n",
      "4. **Search**: The algorithm iterates over the indexed dataset, calculating the similarity score for each item by applying the chosen distance metric to the query and item representations. The items with the highest scores are returned as matches.\n",
      "5. **Ranking**: To improve results, some algorithms apply ranking techniques such as:\n",
      "\t* Sorting: Ordering the top-scoring items based on their similarity scores.\n",
      "\t* Filtering: Removing low-scoring items that are unlikely to be relevant.\n",
      "\n",
      "Similarity search can be applied to various domains, including:\n",
      "\n",
      "* Image retrieval: Finding similar images in a dataset of images.\n",
      "* Text search: Finding similar documents or texts in a corpus.\n",
      "* Audio search: Finding similar audio recordings in a dataset.\n",
      "\n",
      "Some popular similarity search algorithms include:\n",
      "\n",
      "* Annoy (Approximate Nearest Neighbors Oh Yeah!)\n",
      "* Faiss (Facebook AI Similarity Search)\n",
      "* Hnswlib (Hierarchical Navigable Small World Library)\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "Similarity search in a Retrieval-Augmented Generation (RAG) system works by comparing the embedding of a user's question with the embeddings of document chunks using a measure such as cosine similarity. The most relevant chunks are then retrieved and provided as context to the language model, which is instructed to answer the question using only this retrieved context.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: Why does RAG reduce hallucinations?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "I couldn't find any information on \"RAG\" reducing hallucinations. It's possible that it's a medication or treatment that is not well-known or widely researched.\n",
      "\n",
      "However, I can provide some general information on treatments that are known to reduce hallucinations. For example:\n",
      "\n",
      "1. Antipsychotic medications: These medications are commonly used to treat schizophrenia and other psychotic disorders, which often involve hallucinations.\n",
      "2. Benzodiazepines: These medications can be effective in reducing acute anxiety and agitation, which may contribute to hallucinations.\n",
      "3. Anticonvulsants: Some anticonvulsant medications, such as valproate and lamotrigine, have been found to be effective in reducing symptoms of psychosis and hallucinations.\n",
      "\n",
      "If you could provide more context or information about RAG, I may be able to help you better.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "According to the context, RAG reduces hallucinations by providing a user with relevant context from documents that match their query embedding, rather than relying solely on its own internal knowledge generation capabilities. This approach helps to prevent the language model from generating incorrect information and reduces the likelihood of hallucination.\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What are applications of RAG?\n",
      "\n",
      "--- LLaMA Alone ---\n",
      "RAG stands for Radiation Attenuation Glass, also known as Lead Glass. It is a type of glass that contains high concentrations of lead oxide and other metals, which significantly increase its density and ability to absorb radiation.\n",
      "\n",
      "Some of the applications of RAG include:\n",
      "\n",
      "1. **Radiation shielding**: RAG is used in various industries such as nuclear power plants, medical facilities, and research institutions to shield against ionizing radiation.\n",
      "2. **Glass windows for radioactive sources**: RAG glass is used to make windows that allow workers to observe radioactive materials while protecting them from radiation exposure.\n",
      "3. **Medical imaging**: RAG is used in X-ray machines and other medical imaging equipment to reduce radiation exposure to patients and medical staff.\n",
      "4. **Nuclear instruments**: RAG is used in some nuclear instruments, such as spectrometers and counters, to provide radiation shielding and reduce background noise.\n",
      "5. **Radiation detection**: RAG is used in some radiation detectors to increase their sensitivity and accuracy.\n",
      "\n",
      "These applications take advantage of the high density and radiation-absorbing properties of RAG glass.\n",
      "\n",
      "--- LLaMA + RAG ---\n",
      "RAG systems are used in:\n",
      "\n",
      "1. Chatbots\n",
      "2. Knowledge assistants\n",
      "3. Enterprise search\n",
      "4. Research applications\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUESTION:\", q)\n",
    "    \n",
    "    no_rag_answer = llama_no_rag(q)\n",
    "    rag_answer = llama_with_rag(q)\n",
    "    \n",
    "    print(\"\\n--- LLaMA Alone ---\")\n",
    "    print(no_rag_answer)\n",
    "    \n",
    "    print(\"\\n--- LLaMA + RAG ---\")\n",
    "    print(rag_answer)\n",
    "    \n",
    "    results.append({\n",
    "        \"Question\": q,\n",
    "        \"LLaMA Alone\": no_rag_answer,\n",
    "        \"LLaMA + RAG\": rag_answer\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38f86181-9d8d-43e0-a61e-ab1e2e573b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>LLaMA Alone</th>\n",
       "      <th>LLaMA + RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Artificial Intelligence?</td>\n",
       "      <td>Artificial Intelligence (AI) refers to the dev...</td>\n",
       "      <td>Artificial Intelligence (AI) refers to the sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Natural Language Processing?</td>\n",
       "      <td>Natural Language Processing (NLP) is a subfiel...</td>\n",
       "      <td>Natural Language Processing (NLP) refers to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are Large Language Models?</td>\n",
       "      <td>A Large Language Model (LLM) is a type of arti...</td>\n",
       "      <td>According to the provided context, Large Langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is hallucination in LLMs?</td>\n",
       "      <td>In Large Language Models (LLMs), a hallucinati...</td>\n",
       "      <td>In Large Language Models (LLMs), hallucination...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Retrieval-Augmented Generation?</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a tech...</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is chunking used in RAG systems?</td>\n",
       "      <td>Chunking is used in RAG (Resource Allocation G...</td>\n",
       "      <td>Chunking is used in RAG systems to divide docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are embeddings?</td>\n",
       "      <td>Embeddings are a fundamental concept in natura...</td>\n",
       "      <td>In a Retrieval-Augmented Generation (RAG) syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does similarity search work?</td>\n",
       "      <td>Similarity search is a type of search algorith...</td>\n",
       "      <td>Similarity search in a Retrieval-Augmented Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Why does RAG reduce hallucinations?</td>\n",
       "      <td>I couldn't find any information on \"RAG\" reduc...</td>\n",
       "      <td>According to the context, RAG reduces hallucin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are applications of RAG?</td>\n",
       "      <td>RAG stands for Radiation Attenuation Glass, al...</td>\n",
       "      <td>RAG systems are used in:\\n\\n1. Chatbots\\n2. Kn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Question  \\\n",
       "0         What is Artificial Intelligence?   \n",
       "1     What is Natural Language Processing?   \n",
       "2          What are Large Language Models?   \n",
       "3           What is hallucination in LLMs?   \n",
       "4  What is Retrieval-Augmented Generation?   \n",
       "5     Why is chunking used in RAG systems?   \n",
       "6                     What are embeddings?   \n",
       "7         How does similarity search work?   \n",
       "8      Why does RAG reduce hallucinations?   \n",
       "9            What are applications of RAG?   \n",
       "\n",
       "                                         LLaMA Alone  \\\n",
       "0  Artificial Intelligence (AI) refers to the dev...   \n",
       "1  Natural Language Processing (NLP) is a subfiel...   \n",
       "2  A Large Language Model (LLM) is a type of arti...   \n",
       "3  In Large Language Models (LLMs), a hallucinati...   \n",
       "4  Retrieval-Augmented Generation (RAG) is a tech...   \n",
       "5  Chunking is used in RAG (Resource Allocation G...   \n",
       "6  Embeddings are a fundamental concept in natura...   \n",
       "7  Similarity search is a type of search algorith...   \n",
       "8  I couldn't find any information on \"RAG\" reduc...   \n",
       "9  RAG stands for Radiation Attenuation Glass, al...   \n",
       "\n",
       "                                         LLaMA + RAG  \n",
       "0  Artificial Intelligence (AI) refers to the sim...  \n",
       "1  Natural Language Processing (NLP) refers to en...  \n",
       "2  According to the provided context, Large Langu...  \n",
       "3  In Large Language Models (LLMs), hallucination...  \n",
       "4  Retrieval-Augmented Generation (RAG) is a meth...  \n",
       "5  Chunking is used in RAG systems to divide docu...  \n",
       "6  In a Retrieval-Augmented Generation (RAG) syst...  \n",
       "7  Similarity search in a Retrieval-Augmented Gen...  \n",
       "8  According to the context, RAG reduces hallucin...  \n",
       "9  RAG systems are used in:\\n\\n1. Chatbots\\n2. Kn...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"rag_comparison_results.csv\", index=False)\n",
    "\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
